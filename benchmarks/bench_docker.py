#!/usr/bin/env python3
import os
import re
import sys
import argparse
import subprocess
import datetime
import csv
import shutil
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- Configuration ---
# The path inside the container where the host's CWD will be mounted.
CONTAINER_WORK_DIR = "/work"

def check_tools():
    """Check if required host tools (docker, time, timeout) exist."""
    if not shutil.which("docker"):
        print("Error: 'docker' command not found. Is Docker installed and in your PATH?")
        sys.exit(1)
    # The check for time/timeout is now implicitly handled by the Docker container.
    # We assume the specified image has them. A good practice would be to ensure
    # the Docker image is built with 'coreutils' and 'time' packages.
    print("[INFO] Docker command found.")

def get_inner_benchmark_command(script_path, extra_args, instance_path):
    """
    Constructs the benchmark command to be run *inside* the container.
    - Assumes 'python3' for .py files.
    - Uses container-relative paths.
    """
    cmd = []
    
    # Use 'python3' for .py scripts, assuming it's in the container's PATH
    if script_path.endswith('.py'):
        cmd.append('python3')
        cmd.append(script_path)
    else:
        # Assumes the script is an executable in the container's PATH or at the specified path
        cmd.append(script_path)
    
    if extra_args:
        cmd.extend(extra_args)
        
    cmd.append(instance_path)
    return cmd

def parse_time_output(stats_file):
    """
    Parses the output file generated by GNU time.
    Format used: %e,%M,%x (Real Time, Max RSS, Exit Code)
    """
    try:
        with open(stats_file, 'r') as f:
            content = f.read().strip()
            # Only use last line.
            content = content.splitlines()[-1]
            parts = content.split(',')
            if len(parts) >= 3:
                return {
                    'time': parts[0], # Seconds
                    'memory': parts[1], # Kilobytes
                    'exit_code': parts[2]
                }
    except Exception as e:
        print(f"\n[Warning] Could not parse time stats from {stats_file}: {e}")
    
    return {'time': '0', 'memory': '0', 'exit_code': '-1'}

def sanitize_for_container_name(name):
    """Sanitizes a string to be a valid Docker container name component."""
    name = re.sub(r'[^a-zA-Z0-9_.-]', '_', name)
    return name

def process_single_file(
    filename,
    file_index,
    total_files,
    timestamp,
    args,
    pass_through_args,
    host_cwd,
    output_dir,
    running_containers,
    containers_lock,
    print_lock
):
    """
    Process a single benchmark file. Returns a dict with results.
    
    Args:
        filename: Name of the benchmark file
        file_index: Index of the file (0-based) for progress display
        total_files: Total number of files being processed
        timestamp: Timestamp string for unique container naming
        args: Parsed command line arguments
        pass_through_args: Arguments to pass to the benchmark script
        host_cwd: Absolute path of the host's current working directory
        output_dir: Directory where output files are stored
        running_containers: Set of currently running container names (thread-safe)
        containers_lock: Lock for accessing running_containers
        print_lock: Lock for thread-safe printing
        
    Returns:
        dict: Contains 'filename', 'status', 'runtime', 'memory', 'exit_code'
    """
    
    # --- Unique Container Name ---
    sanitized_filename = sanitize_for_container_name(os.path.splitext(filename)[0])
    container_name = f"benchmark-{timestamp}-{sanitized_filename}"
    
    # --- Path Translation ---
    def to_container_path(host_path):
        abs_host_path = os.path.abspath(host_path)
        relative_path = os.path.relpath(abs_host_path, host_cwd)
        return os.path.join(CONTAINER_WORK_DIR, relative_path)
    
    container_instance_path = to_container_path(os.path.join(args.folder, filename))
    if args.user_script:
        container_script_path = to_container_path(args.user_script)
    else:
        container_script_path = args.docker_script
    container_log_path = to_container_path(os.path.join(output_dir, f"{filename}.log"))
    container_stats_path = to_container_path(os.path.join(output_dir, f"{filename}.stats"))
    
    # Host path for parsing the stats file after the run
    host_stats_path = os.path.join(output_dir, f"{filename}.stats")
    
    # --- Command Construction ---
    inner_cmd = get_inner_benchmark_command(
        container_script_path,
        pass_through_args,
        container_instance_path
    )
    
    container_shell_cmd = (
        f'/usr/bin/time -f "%e,%M,%x" -o {container_stats_path} '
        f'timeout {args.timeout} {" ".join(inner_cmd)} '
        f'> {container_log_path} 2>&1'
    )
    
    docker_cmd = [
        'docker', 'run',
        '--rm',
        '--init',
        '--name', container_name,
        '-v', f'{host_cwd}:{CONTAINER_WORK_DIR}',
        '-w', CONTAINER_WORK_DIR,
        args.docker_image,
        '/bin/sh', '-c', container_shell_cmd
    ]
    
    with print_lock:
        print(f"[{file_index+1}/{total_files}] Running {filename}...")
    
    status = "UNKNOWN"
    runtime = 0
    memory = 0
    exit_code = 0
    
    try:
        # Register this container as running
        with containers_lock:
            running_containers.add(container_name)
        
        # Use Popen to get control over the running process
        proc = subprocess.Popen(docker_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        proc.wait()
        
        # Unregister the container
        with containers_lock:
            running_containers.discard(container_name)
        
        # Parse results
        stats = parse_time_output(host_stats_path)
        runtime = stats['time']
        memory = stats['memory']
        exit_code = int(stats['exit_code'])
        
        if exit_code == 124:
            status = "TIMEOUT"
            with print_lock:
                print(f"[{file_index+1}/{total_files}] Finished {filename} -> TIMEOUT ({runtime}s)")
        elif exit_code == 0:
            status = "OK"
            with print_lock:
                print(f"[{file_index+1}/{total_files}] Finished {filename} -> OK ({runtime}s)")
        else:
            status = "ERROR"
            with print_lock:
                print(f"[{file_index+1}/{total_files}] Finished {filename} -> ERROR (Code: {exit_code})")
    
    except Exception as e:
        with containers_lock:
            running_containers.discard(container_name)
        with print_lock:
            print(f"\n[ERROR] Docker execution failed: {e}")
        status = "SYSTEM_ERROR"
    
    return {
        'filename': filename,
        'status': status,
        'runtime': runtime,
        'memory': memory,
        'exit_code': exit_code
    }

def main():
    parser = argparse.ArgumentParser(
        description="Automated Benchmark Runner using Docker",
        usage="%(prog)s --docker-image IMAGE --timeout TIMEOUT --folder FOLDER --script SCRIPT [options] [-- script_args ...]"
    )
    
    # Required arguments
    parser.add_argument('--docker-image', required=True, help="Name of the Docker image to use for benchmarking")
    parser.add_argument('--timeout', required=True, help="Timeout string (e.g., 60s, 10m)")
    parser.add_argument('--folder', required=True, help="Path to folder with benchmark instances")    
    
    # Optional arguments
    parser.add_argument('--match', help="Only consider files matching this regular expression")
    parser.add_argument('--docker-script', help="Path to the benchmark script/binary in the docker container; Defaults to `/run.sh`.")
    parser.add_argument('--user-script', help="Path to the benchmark script/binary relative to CWD. User script and docker script cannot be set at the same time.")
    parser.add_argument('--parallel', type=int, default=1, help="Number of parallel benchmark instances to run (default: 1)")
    
    # Catch-all for arguments to be passed to the benchmark script
    parser.add_argument('script_args', nargs=argparse.REMAINDER, help="Arguments passed to the benchmark script")

    args = parser.parse_args()
    
    # 0. Pre-flight checks
    check_tools()
    
    if args.docker_script and args.user_script:
        print(f"[FATAL] Cannot specify both a Docker script and user script.")
        sys.exit(1)

    if not args.docker_script and not args.user_script:
        args.docker_script = "/run.sh"
    
    if args.parallel < 1:
        print(f"[FATAL] --parallel must be at least 1.")
        sys.exit(1)

    pass_through_args = args.script_args
    if pass_through_args and pass_through_args[0] == '--':
        pass_through_args = pass_through_args[1:]

    # 1. Setup Output Directory
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    output_dir = f"run_{timestamp}"
    
    try:
        os.makedirs(output_dir, exist_ok=True)
        print(f"[INFO] Created output directory: {output_dir}")
    except OSError as e:
        print(f"[FATAL] Could not create output directory: {e}")
        sys.exit(1)

    # 2. Load and Sort Files
    if not os.path.isdir(args.folder):
        print(f"[FATAL] Benchmark folder not found: {args.folder}")
        sys.exit(1)

    files = [f for f in os.listdir(args.folder) if os.path.isfile(os.path.join(args.folder, f))]
    
    if args.match:
        files = [f for f in files if bool(re.fullmatch(args.match, os.path.basename(f)))]
    
    files.sort()
    
    if not files:
        print(f"[WARN] No files found in {args.folder} matching criteria.")
        sys.exit(0)

    print(f"[INFO] Found {len(files)} instances to process.")
    print(f"[INFO] Docker image: {args.docker_image}")
    print(f"[INFO] Timeout set to: {args.timeout}")
    print(f"[INFO] Parallel jobs: {args.parallel}")
    print(f"[INFO] Benchmark script: { args.docker_script if args.user_script is None else args.user_script }")
    if pass_through_args:
        print(f"[INFO] Extra args: {' '.join(pass_through_args)}")

    # 3. Prepare CSV and path variables
    csv_path = os.path.join(output_dir, "__results.csv")
    csv_header = ["Instance", "Status", "Runtime_sec", "Memory_KB", "ExitCode"]
    host_cwd = os.path.abspath(os.getcwd())
    
    # Thread-safety mechanisms
    running_containers = set()
    containers_lock = threading.Lock()
    csv_lock = threading.Lock()
    print_lock = threading.Lock()

    # 4. Processing Loop with Parallel Execution
    try:
        with open(csv_path, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(csv_header)
            csvfile.flush()
            
            # Use ThreadPoolExecutor for parallel execution
            with ThreadPoolExecutor(max_workers=args.parallel) as executor:
                # Submit all jobs
                future_to_file = {}
                for i, filename in enumerate(files):
                    future = executor.submit(
                        process_single_file,
                        filename=filename,
                        file_index=i,
                        total_files=len(files),
                        timestamp=timestamp,
                        args=args,
                        pass_through_args=pass_through_args,
                        host_cwd=host_cwd,
                        output_dir=output_dir,
                        running_containers=running_containers,
                        containers_lock=containers_lock,
                        print_lock=print_lock
                    )
                    future_to_file[future] = filename
                
                # Process completed jobs as they finish
                for future in as_completed(future_to_file):
                    try:
                        result = future.result()
                        
                        # Write result to CSV in a thread-safe manner
                        with csv_lock:
                            writer.writerow([
                                result['filename'],
                                result['status'],
                                result['runtime'],
                                result['memory'],
                                result['exit_code']
                            ])
                            csvfile.flush()
                    
                    except Exception as e:
                        filename = future_to_file[future]
                        with print_lock:
                            print(f"\n[ERROR] Unexpected error processing {filename}: {e}")
                        with csv_lock:
                            writer.writerow([filename, "SYSTEM_ERROR", 0, 0, -1])
                            csvfile.flush()

    except KeyboardInterrupt:
        print("\n[INFO] Benchmark interrupted by user. Stopping all running containers...")
        
        # Get a snapshot of running containers
        with containers_lock:
            containers_to_stop = list(running_containers)
        
        # Stop all running containers
        for container_name in containers_to_stop:
            try:
                subprocess.run(
                    ['docker', 'stop', container_name],
                    check=False,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    timeout=5
                )
            except Exception:
                pass  # Best effort cleanup
        
        print("[INFO] All containers stopped. Exiting...")
        sys.exit(130)

    print(f"\n[INFO] Benchmarking complete. Results saved to {csv_path}")

if __name__ == "__main__":
    main()